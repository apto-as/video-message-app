#!/usr/bin/env python3
"""
Trinitas-Core ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åŒæœŸã‚·ã‚¹ãƒ†ãƒ 
åˆ†æ•£ã—ãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹
"""

import asyncio
import json
import logging
import shutil
from pathlib import Path
from datetime import datetime
import sys

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TrinityMetadataSync:
    """ä¸‰ä½ä¸€ä½“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åŒæœŸã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        
        # 2ã¤ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å ´æ‰€
        self.backend_storage = self.project_root / "backend" / "storage" / "voices"
        self.data_storage = self.project_root / "data" / "backend" / "storage" / "voices"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        self.backend_metadata = self.backend_storage / "voices_metadata.json"
        self.data_metadata = self.data_storage / "voices_metadata.json"
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.backend_profiles = self.backend_storage / "profiles"
        self.data_profiles = self.data_storage / "profiles"
    
    def read_metadata(self, metadata_file: Path) -> dict:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"profiles": {}}
    
    def write_metadata(self, metadata_file: Path, data: dict):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿"""
        metadata_file.parent.mkdir(parents=True, exist_ok=True)
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    async def springfield_strategic_analysis(self):
        """Springfield: æˆ¦ç•¥çš„åˆ†æ"""
        logger.info("=== Springfield: æˆ¦ç•¥çš„åˆ†æ ===")
        
        # ä¸¡æ–¹ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸çŠ¶æ³ã‚’ç¢ºèª
        backend_exists = self.backend_storage.exists()
        data_exists = self.data_storage.exists()
        
        logger.info(f"Backend storage: {backend_exists} - {self.backend_storage}")
        logger.info(f"Data storage: {data_exists} - {self.data_storage}")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        backend_metadata = self.read_metadata(self.backend_metadata)
        data_metadata = self.read_metadata(self.data_metadata)
        
        backend_profiles = set(backend_metadata.get('profiles', {}).keys())
        data_profiles = set(data_metadata.get('profiles', {}).keys())
        
        logger.info(f"Backend ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {len(backend_profiles)}å€‹ - {backend_profiles}")
        logger.info(f"Data ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {len(data_profiles)}å€‹ - {data_profiles}")
        
        # å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
        backend_actual = set()
        data_actual = set()
        
        if self.backend_profiles.exists():
            for profile_dir in self.backend_profiles.iterdir():
                if profile_dir.is_dir() and (profile_dir / "profile.json").exists():
                    backend_actual.add(profile_dir.name)
        
        if self.data_profiles.exists():
            for profile_dir in self.data_profiles.iterdir():
                if profile_dir.is_dir() and (profile_dir / "profile.json").exists():
                    data_actual.add(profile_dir.name)
        
        logger.info(f"Backend å®Ÿãƒ•ã‚¡ã‚¤ãƒ«: {len(backend_actual)}å€‹ - {backend_actual}")
        logger.info(f"Data å®Ÿãƒ•ã‚¡ã‚¤ãƒ«: {len(data_actual)}å€‹ - {data_actual}")
        
        return {
            'backend_metadata': backend_metadata,
            'data_metadata': data_metadata,
            'backend_actual': backend_actual,
            'data_actual': data_actual,
            'all_profiles': backend_actual | data_actual | backend_profiles | data_profiles
        }
    
    async def krukai_technical_migration(self, analysis_result: dict):
        """Krukai: æŠ€è¡“çš„çµ±åˆ"""
        logger.info("=== Krukai: æŠ€è¡“çš„çµ±åˆ ===")
        
        # çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        unified_metadata = {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "profiles": {}
        }
        
        # æ—¢å­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
        for source_name, metadata in [
            ("backend", analysis_result['backend_metadata']),
            ("data", analysis_result['data_metadata'])
        ]:
            for profile_id, profile_data in metadata.get('profiles', {}).items():
                if profile_id not in unified_metadata['profiles']:
                    unified_metadata['profiles'][profile_id] = profile_data
                    logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {profile_id} from {source_name}")
        
        # å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸è¶³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¾©å…ƒ
        for profile_dir_path in [self.backend_profiles, self.data_profiles]:
            if not profile_dir_path.exists():
                continue
                
            for profile_dir in profile_dir_path.iterdir():
                if not profile_dir.is_dir():
                    continue
                    
                profile_id = profile_dir.name
                profile_json = profile_dir / "profile.json"
                
                if not profile_json.exists():
                    continue
                
                if profile_id in unified_metadata['profiles']:
                    continue
                
                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                try:
                    with open(profile_json, 'r', encoding='utf-8') as f:
                        profile_data = json.load(f)
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ã«ç°¡ç•¥åŒ–
                    unified_metadata['profiles'][profile_id] = {
                        'name': profile_data.get('name', f'recovered-{profile_id}'),
                        'provider': profile_data.get('provider', 'openvoice'),
                        'status': profile_data.get('status', 'ready'),
                        'created_at': profile_data.get('created_at', datetime.now().isoformat()),
                        'updated_at': datetime.now().isoformat(),
                        'storage_path': str(profile_dir),
                        'reference_audio_path': profile_data.get('reference_audio_path'),
                        'embedding_path': profile_data.get('embedding_path')
                    }
                    
                    logger.info(f"å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¾©å…ƒ: {profile_id} - {profile_data.get('name')}")
                    
                except Exception as e:
                    logger.error(f"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å¾©å…ƒã‚¨ãƒ©ãƒ¼ {profile_id}: {str(e)}")
        
        logger.info(f"çµ±åˆå®Œäº†: {len(unified_metadata['profiles'])}å€‹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«")
        return unified_metadata
    
    async def vector_security_validation(self, unified_metadata: dict):
        """Vector: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼"""
        logger.info("=== Vector: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ ===")
        
        risks = []
        warnings = []
        validated_profiles = {}
        
        for profile_id, profile_data in unified_metadata['profiles'].items():
            profile_risks = []
            profile_warnings = []
            
            # ãƒ‘ã‚¹æ¤œè¨¼
            paths = [
                profile_data.get('storage_path'),
                profile_data.get('reference_audio_path'),
                profile_data.get('embedding_path')
            ]
            
            for path in paths:
                if not path:
                    continue
                    
                # ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ¤œè¨¼
                if '..' in path or path.startswith('/tmp'):
                    profile_risks.append(f"å±é™ºãªãƒ‘ã‚¹: {path}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
                if not Path(path).exists():
                    profile_warnings.append(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨: {path}")
            
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«IDæ¤œè¨¼
            if not profile_id.startswith('openvoice_'):
                profile_risks.append(f"ä¸æ­£ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ID: {profile_id}")
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ç¢ºèª
            required_fields = ['name', 'provider', 'status']
            for field in required_fields:
                if not profile_data.get(field):
                    profile_warnings.append(f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸è¶³: {field}")
            
            # å®‰å…¨ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿çµ±åˆ
            if not profile_risks:
                validated_profiles[profile_id] = profile_data
                if profile_warnings:
                    logger.warning(f"è­¦å‘Šã‚ã‚Šï¼ˆç¶™ç¶šï¼‰: {profile_id} - {profile_warnings}")
            else:
                logger.error(f"å±é™ºãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé™¤å¤–ï¼‰: {profile_id} - {profile_risks}")
                risks.extend(profile_risks)
            
            warnings.extend(profile_warnings)
        
        logger.info(f"æ¤œè¨¼å®Œäº†: {len(validated_profiles)}/{len(unified_metadata['profiles'])}å€‹ãŒå®‰å…¨")
        logger.info(f"ãƒªã‚¹ã‚¯: {len(risks)}ä»¶, è­¦å‘Š: {len(warnings)}ä»¶")
        
        return {
            'validated_profiles': validated_profiles,
            'risks': risks,
            'warnings': warnings,
            'security_score': max(0, 100 - len(risks) * 20 - len(warnings) * 5)
        }
    
    async def trinity_unified_deployment(self, validated_data: dict):
        """ä¸‰ä½ä¸€ä½“çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤"""
        logger.info("=== Trinitasçµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤ ===")
        
        # æœ€çµ‚çµ±åˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        final_metadata = {
            "version": "1.0",
            "created_at": datetime.now().isoformat(),
            "profiles": validated_data['validated_profiles'],
            "metadata": {
                "sync_timestamp": datetime.now().isoformat(),
                "total_profiles": len(validated_data['validated_profiles']),
                "security_score": validated_data['security_score'],
                "trinitas_core_version": "2.0"
            }
        }
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        backup_dir = self.backend_storage.parent / "backup" / f"metadata_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        if self.backend_metadata.exists():
            shutil.copy2(self.backend_metadata, backup_dir / "backend_metadata.json")
        if self.data_metadata.exists():
            shutil.copy2(self.data_metadata, backup_dir / "data_metadata.json")
        
        logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_dir}")
        
        # æ­£è¦ä½ç½®ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é…ç½®
        self.write_metadata(self.backend_metadata, final_metadata)
        logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿é…ç½®: {self.backend_metadata}")
        
        # data/backend ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚æ›´æ–°ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        self.write_metadata(self.data_metadata, final_metadata)
        logger.info(f"äº’æ›ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°: {self.data_metadata}")
        
        # æœ€çµ‚ç¢ºèª
        verification_metadata = self.read_metadata(self.backend_metadata)
        verification_count = len(verification_metadata.get('profiles', {}))
        
        logger.info(f"âœ… çµ±åˆå®Œäº†: {verification_count}å€‹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆ©ç”¨å¯èƒ½")
        
        return {
            'success': True,
            'profile_count': verification_count,
            'backup_location': str(backup_dir),
            'primary_metadata': str(self.backend_metadata)
        }
    
    async def execute_full_sync(self):
        """å®Œå…¨åŒæœŸå®Ÿè¡Œ"""
        logger.info("=== Trinitas-Core ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åŒæœŸé–‹å§‹ ===")
        
        try:
            # 1. Springfield: æˆ¦ç•¥åˆ†æ
            analysis = await self.springfield_strategic_analysis()
            
            # 2. Krukai: æŠ€è¡“çµ±åˆ
            unified_metadata = await self.krukai_technical_migration(analysis)
            
            # 3. Vector: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼
            validation_result = await self.vector_security_validation(unified_metadata)
            
            # 4. Trinity: çµ±åˆãƒ‡ãƒ—ãƒ­ã‚¤
            deployment_result = await self.trinity_unified_deployment(validation_result)
            
            logger.info("=== åŒæœŸå®Œäº† ===")
            logger.info(f"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {deployment_result['profile_count']}")
            logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {deployment_result['backup_location']}")
            logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {deployment_result['primary_metadata']}")
            
            return deployment_result
            
        except Exception as e:
            logger.error(f"åŒæœŸã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    sync_system = TrinityMetadataSync()
    result = await sync_system.execute_full_sync()
    
    if result['success']:
        logger.info("ğŸŒ¸ ä¸‰ä½ä¸€ä½“åŒæœŸæˆåŠŸ - ã‚«ãƒ•ã‚§ãƒ»ã‚ºãƒƒã‚±ãƒ­ã§ãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™")
    else:
        logger.error(f"âŒ åŒæœŸå¤±æ•—: {result.get('error')}")

if __name__ == "__main__":
    asyncio.run(main())